{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Embeddings\n",
    "\n",
    "## Setup\n",
    "\n",
    "The notebook requires ATLAS URI and LLAMA API key. Along with that, in the stackup_ai database's collection zendesk_data collection, we need to create a search index (explained later) where our LLM will fetch the embeddings to respond to a user's query as accurate as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script sets up a connection to a MongoDB Atlas cluster using the ATLAS_URI environment variable.\n",
    "It also configures logging to print messages at the INFO level to the console.\n",
    "The script first imports necessary modules and sets up logging.\n",
    "It then loads environment variables from a .env file using the dotenv_values function.\n",
    "The ATLAS_URI environment variable is retrieved and used to create a MongoDB client connection.\n",
    "If the ATLAS_URI environment variable is not set, the script raises an exception.\n",
    "\"\"\"\n",
    "import sys, os\n",
    "import logging\n",
    "import pymongo\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Load settings from .env file\n",
    "from dotenv import find_dotenv, dotenv_values\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "config = dotenv_values(find_dotenv())\n",
    "\n",
    "ATLAS_URI = config.get('ATLAS_URI')\n",
    "\n",
    "if not ATLAS_URI:\n",
    "    raise Exception (\"'ATLAS_URI' is not set.  Please set it above to continue...\")\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(ATLAS_URI)\n",
    "print (\"Atlas client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Constants for the Zendesk data processing pipeline.\n",
    "DB_NAME: The name of the database to store the processed data.\n",
    "COLLECTION_NAME: The name of the MongoDB collection to store the Zendesk data.\n",
    "INDEX_NAME: The name of the index to be created on the embeddings field in the Zendesk data collection.\n",
    "\"\"\"\n",
    "DB_NAME = 'stackup_ai'\n",
    "COLLECTION_NAME = 'zendesk_data'\n",
    "INDEX_NAME = 'index_embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sets the environment variable 'LLAMA_INDEX_CACHE_DIR' to the path 'cache' located in the parent directory of the current script.\n",
    "This is likely used to specify a directory for caching data related to the LLaMA index,\n",
    "which is a data structure used for efficient retrieval of information from large language models.\n",
    "'''\n",
    "os.environ['LLAMA_INDEX_CACHE_DIR'] = os.path.join(os.path.abspath('../'), 'cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count before delete : 26\n",
      "Deleted docs : 26\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Deletes all documents from the specified MongoDB collection.\n",
    "This code connects to a MongoDB database, retrieves the specified collection, and then deletes all documents from that collection.\n",
    "It first prints the total number of documents in the collection, then calls the `delete_many()` method to delete all documents,\n",
    "and finally prints the number of deleted documents.\n",
    "This function is intended to be used for maintenance or testing purposes,as it will permanently remove all data from the specified collection.\n",
    "Caution should be exercised when using this code in a production environment.\n",
    "\"\"\"\n",
    "database = mongodb_client[DB_NAME]\n",
    "collection = database [COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initializes a HuggingFaceEmbedding model and a ServiceContext with the specified embedding model.\n",
    "The HuggingFaceEmbedding model is used for generating embeddings of text data.\n",
    "The ServiceContext is a container for various services used by the LlamaIndex library, including the embedding model.\n",
    "Args:\n",
    "    model_name (str): The name of the HuggingFace model to use for generating embeddings.\n",
    "Returns:\n",
    "    ServiceContext: A ServiceContext instance with the specified embedding model.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports the MongoDBAtlasVectorSearch class from the llama_index.vector_stores.mongodb module.\n",
    "The MongoDBAtlasVectorSearch class provides a vector store implementation that uses MongoDB Atlas as the backend.\n",
    "It can be used to store and retrieve vector embeddings and associated metadata.\n",
    "\"\"\"\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 vector_index_name  = INDEX_NAME,\n",
    "                                 ## The following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17 chunks from './data/'\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reads data files from a directory using the SimpleDirectoryReader class from the llama_index.core module.\n",
    "The `data_dir` variable specifies the directory path where the data files are located.\n",
    "The `SimpleDirectoryReader` class is used to load all the data files in the directory and return a list of documents.\n",
    "The number of chunks (documents) loaded from the directory is printed to the console.\n",
    "\"\"\"\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "data_dir = './data/'\n",
    "\n",
    "## This reads one doc\n",
    "# docs = SimpleDirectoryReader(\n",
    "#     input_files=[\"./data/10k/uber_2021.pdf\"]\n",
    "\n",
    "## This reads an entire directory\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:10<00:00, 10.35s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:03<00:00,  3.39s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:02<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Creates a VectorStoreIndex from the provided documents, using the given storage and service contexts.\n",
    "Args:\n",
    "    docs (List[Document]): A list of documents to create the index from.\n",
    "    storage_context (StorageContext): The storage context to use for the index.\n",
    "    service_context (ServiceContext): The service context to use for the index.\n",
    "Returns:\n",
    "    VectorStoreIndex: The created VectorStoreIndex.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to Atlas MongoDB > stackup_ai > zendesk_data and create a search index with name `index_embeddings` with following definition\n",
    "```json\n",
    "{\n",
    "  \"fields\": [\n",
    "    {\n",
    "      \"numDimensions\": 384,\n",
    "      \"path\": \"embedding\",\n",
    "      \"similarity\": \"cosine\",\n",
    "      \"type\": \"vector\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:2 prompts are loaded, with the keys: ['query', 'text']\n",
      "2 prompts are loaded, with the keys: ['query', 'text']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initializes a LlamaAPI instance with the provided LLAMA_API key.\n",
    "The LlamaAPI class is used to interact with the Llama language model API.\n",
    "It provides methods for generating text, embedding text, and other language model-related operations.\n",
    "Args:\n",
    "    api_key (str): The API key for the Llama language model API.\n",
    "\"\"\"\n",
    "from llama_index.llms.llama_api import LlamaAPI\n",
    "\n",
    "LLAMA_API = config.get('LLAMA_API')\n",
    "llm = LlamaAPI(api_key=LLAMA_API)\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializes a ServiceContext object with the provided embed_model and llm parameters.\n",
    "The ServiceContext class is used to configure the various services used by the LlamaIndex library,\n",
    "such as the embedding model and language model.\n",
    "This code sets the default embed_model and llm parameters for the ServiceContext, which can be used throughout the application.\n",
    "\"\"\"\n",
    "from llama_index.core import ServiceContext\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initializes a MongoDBAtlasVectorSearch object and creates a StorageContext and VectorStoreIndex from it.\n",
    "The MongoDBAtlasVectorSearch object is configured with the provided MongoDB client, database name, collection name, and index name.\n",
    "The default values for the embedding_key, text_key, and metadata_ parameters are used.\n",
    "The StorageContext is created from the vector store, and the VectorStoreIndex is created from the vector store and the service context.\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 vector_index_name  = INDEX_NAME,\n",
    "                                 ## the following columns are set to default values\n",
    "                                 # embedding_key = 'embedding', text_key = 'text', metadata_= 'metadata',\n",
    "                                 )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 18.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<b>The StackUp [bounty](https://earn.stackup.dev/) program offers an additional opportunity for Stackies to engage in more advanced learning activities with higher expectations for their output. This program presents a new level of challenge compared to quests, allowing Stackies to tackle more complex challenges in exchange for a larger reward amount.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Queries the index using the provided query string and displays the response as Markdown-formatted text.\n",
    "Args:\n",
    "    query (str): The query string to be used for searching the index.\n",
    "Returns:\n",
    "    None\n",
    "\"\"\"\n",
    "from IPython.display import Markdown\n",
    "\n",
    "response = index.as_query_engine().query(\"What is bounty? Explain in detail.\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n",
    "# pprint(response, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7607830762863159\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Prints the score of the first source node in the response.\n",
    "# \"\"\"\n",
    "print(f\"Score: {response.source_nodes[0].score}\")\n",
    "# print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
